\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
% \PassOptionsToPackage{numbers, compress}{natbib}
% before loading rl_project.

% to compile a camera-ready version, add the [final] option, e.g.:
 \usepackage[final]{rl_project}

% to avoid loading the natbib package, add option nonatbib:
% \usepackage[nonatbib]{rl_project}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{graphicx}
\bibliographystyle{bathx}  

% Give your project report an appropriate title!

\title{RL Project Template}


% The \author macro works with any number of authors. There are two
% commands used to separate the names and addresses of multiple
% authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to
% break the lines. Using \AND forces a line break at that point. So,
% if LaTeX puts 3 of 4 authors names on the first line, and the last
% on the second line, try using \AND instead of \And before the third
% author name.

\author{
  Alireza Samar, Arnold Gomes, Danny Demarco, 
  \\
  Department of Computer Science\\
  University of Bath\\
  Bath, BA2 7AY \\
  \texttt{\{as4731, ag2342, dd480, rdw41\}@bath.ac.uk} \\
  %% examples of more authors
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
}

\begin{document}

\maketitle

\section{Problem Definition}
A clear, precise and concise description of your chosen problem,
including the states, actions, transition dynamics, and the reward function. You will lose
marks for an unclear, incorrect, or incomplete problem definition.

Application of reinforcement learning methods to modern artificial intelligence applications are both interesting and challenging, especially when the environment is unknown and there may be delayed rewards. This group project is an investigation into some of these methods and will attempt to train a number of agents to land a simulated spacecraft on the moon and compare their results for efficacy. To enable this the choice was made to make use of the LunarLander-v2 environment provided by OpenAI in their Gym package. The environment provides a classic rocket trajectory problem in which the rocket, starting at an elevated position is tasked with landing safely on a designated landing pad. There are two options in using this environment, discrete or continuous and we have chosen to use the discrete option. The 2d environment consists of the moon on the lower boundary with a section of it designated as the landing pad limited by the area between 2 flags, and an area of space above it in which the rocket may fly. The rocket itself is fitted with 2 legs to its base and three engines to provide thrust vertically or horizontally. To understand how the environment functions in more depth we can break it down into 4 main sections:


\begin{enumerate}
  \item States
  \begin{enumerate}
    \item Starting state\\
    Each episode begins with the rocket in the same position; at the top, center of the viewport. The variable factor is that there is a random force applied to the rockets center of mass at the outset.
    
    \item Observation space\\
     During an episode there are 8 possible states for the rocket to be in and consist of an 'x' and 'y' coordinate to denote its position, the linear velocity in both 'x' and 'y', the current angle of the rocket, the angular velocity, and two boolean values that represent whether each leg of the rocket is in contact with the ground or not. The landing pad is designated with (0,0) for its 'x' and 'y' coordinates.
  \end{enumerate}
  \item Actions\\
    There are 4 discrete actions available at any point: no action, fire right engine (left orientation), fire left engine (right orientation), fire main engine (upward orientation). Fuel is considered infinite and there is no range of acceleration, engines are binary and either on or off.
  
  \item Rewards\\
  Agents are incentivised/decentivised with the following reward structure:\\
  \begin{enumerate}
    \item Positive Rewards\\
    The reward for succesfully moving from the starting state and coming to rest at the landing pad range between 100-140 points depending on the accuracy of the landing. Each leg that is in contact with the ground provides an extra 10 points to incentivise a level approach to the landing pad. Once an episode is succesfully solved, a reward of 200 points is awarded.
    \item Negative Rewards\\
    To incentivise the agent to land on the platform as quickly as possible, negative rewards are given for each use of any of the engines in each frame of an episode of -0.3. If the rocket crashes (touches the moon), it receives -100 points.
  \end{enumerate}

  \item Episodes\\
  Each episode starts in the given starting state and runs until the terminal state is activated. A terminal state is activated in any of the following instances:
  \begin{enumerate}
    \item The rocket moves outside of the viewport (moves too far left, right, or up)
    \item The rocket crashes into the moon (misses the landing platform and hits the lower boundary)
    \item A sleep state is reached, i.e. not in motion and not involved in a collision. This includes once the rocket has succesfully landed on the landing platform.
  \end{enumerate}
\end{enumerate}

With these particulars in mind we can see that our agent is incentivised to direct the rocket from its starting position, to the landing pad with the minimal amount of engine use (fuel) as possible, while keeping the rocket stabilised as much as possible so that its legs may both make contact with the landing pad simultaneously. Incentivisation is produced by a large positive reward for reaching its platform, with bonus points for landing there in an upright position, and negative rewards for moving too far off course or colliding with the moons surface.


\section{Background}
\textbf{TODO: Explain MDPs}
\textbf{TODO: Explain basic RL ; Bellman Equation and Dynamic Programming/Optimization problems, Value Iteration Methods, Policy Gradient Methods}

\section{Method}
A description of the method(s) used to solve your chosen problem, an explanation
of how these methods work (in your own words), and an explanation of why you chose these
specific methods.\\\newline

To understand our chosen method PPO, we must first introduce TRPO. 
\subsection{TRPO}
Trust Region Policy Optimization (TRPO)
\subsection{PPO}

Proximal Policy Optimization is a policy gradient method that alternates between sampling data
through environment interactions and optimizing a ’surrogate’ objective function using ’stochastic
gradient ascent’ \citep{schulman2017proximal}. The PPO objective function makes use of multiple epochs
of minibatch updates. For the lunar lander test case this means a favorable balance between sample
’wall-time, simplicity and complexity’ \citep{schulman2017proximal}.

\section{Results}
A presentation of your results, showing how quickly and how well your agent(s)
learn (i.e. improve their policies). Include informative baselines for comparison (e.g. the best
possible performance, the performance of an average human, or the performance of an agent
that selects actions randomly).

\section{Discussion}
An evaluation of how well you solved your chosen problem.

\section{Future Work}
A discussion of potential future work you would complete if you had more
time.

\section{Personal Experience}
A discussion of your personal experience with the project, such as
difficulties or pleasant surprises you encountered while completing it.


<<<<<<< HEAD
\bibliography{references}

=======
\section*{References}
\small
\normalsize
>>>>>>> 9baaf3218aa764385334b86f3b3431744edc2692
\newpage
\section*{Appendices}
If you have additional content that you would like to include in the appendices, please do so here.
There is no limit to the length of your appendices, but we are not obliged to read them in their entirety while marking. The main body of your report should contain all essential information, and content in the appendices should be clearly referenced where it's needed elsewhere.



Appendices should include (1) a detailed description of the problem domain,
including the states, actions, reward function, and transition dynamics; (2) all experimental
details so that the reader can fully replicate your experiments; and (3) how you selected your
hyperparameters (if applicable).
\subsection*{Appendix A: Example Appendix 1}
\subsection*{Appendix B: Example Appendix 2}

\end{document}
